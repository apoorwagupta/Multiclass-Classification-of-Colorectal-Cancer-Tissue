{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorwagupta/Multiclass-Classification-of-Colorectal-Cancer-Tissue/blob/main/VGG_19_Bottleneck.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do transfer learning, we will remove the last fully connected layer from the model and plug in wer layers there. The \"truncated\" model output is going to be the features that will fill wer \"model\". Those are the bottleneck features.\n",
        "\n",
        "* VGG19 is a pretrain-model over ImageNet catalog that has very good accuracy. \n",
        "\n",
        "* Bottleneck features depends on the model. In this case, we are using VGG19. There are others pre-trained models like VGG16, ResNet-50\n",
        "\n",
        "* It's like we are cutting a model and adding our own layers. Mainly, the output layer to decide what we want to detect, the final output."
      ],
      "metadata": {
        "id": "5utkelrgnyvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsaM4_DOJ7II"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# deep learning libraries\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import applications\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, Dense, Dropout\n",
        "from keras.preprocessing import image\n",
        "\n",
        "import cv2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgOv1jP-J8hi",
        "outputId": "d618f637-cd52-41f1-a09a-2d77d03a0091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAUwHDuSPu3k"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/drive/MyDrive/cancerhistologydataset.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/base_dir') #Extracts the files into the /base_dir folder\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N8ufY3gPy4O",
        "outputId": "b4bfe2fa-ba02-48bc-d5e2-70fa071b8fe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(os.listdir('/base_dir/Kather_texture_2016_image_tiles_5000/Kather_texture_2016_image_tiles_5000'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF9kA3bnP1EE"
      },
      "outputs": [],
      "source": [
        "base_dir = '/base_dir/Kather_texture_2016_image_tiles_5000/Kather_texture_2016_image_tiles_5000'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAFiXJF1P7Mk",
        "outputId": "26480fe3-fae7-458e-f299-a151901bef93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 8 classes in this dataset:\n",
            "['02_STROMA', '05_DEBRIS', '06_MUCOSA', '03_COMPLEX', '07_ADIPOSE', '08_EMPTY', '04_LYMPHO', '01_TUMOR']\n"
          ]
        }
      ],
      "source": [
        "## Get labels of all classes in this dataset\n",
        "img_labels = [i for i in os.listdir(base_dir) if not i.startswith('.')]\n",
        "print('There are {} classes in this dataset:\\n{}'.format(len(img_labels), img_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7CydZTkk79A",
        "outputId": "9b05e935-5e64-4484-ce0e-0cd48198b8e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "pip install split-folders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b88h-LgClG07"
      },
      "outputs": [],
      "source": [
        "import splitfolders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzgro0FSlJ4X",
        "outputId": "8f90733f-79b6-42c1-a08b-26ff93faa1a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying files: 5000 files [00:05, 984.81 files/s] \n"
          ]
        }
      ],
      "source": [
        "splitfolders.ratio(base_dir,output=\"datasetaftersplit\",seed=35, ratio= (0.7,0.15,0.15), group_prefix = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjK-pJeLNSAh",
        "outputId": "fac0ccf1-61ee-429e-a594-9551e9c2877b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 0s 0us/step\n",
            "80150528/80134624 [==============================] - 0s 0us/step\n",
            "Found 3496 images belonging to 8 classes.\n",
            "3496\n",
            "{'01_TUMOR': 0, '02_STROMA': 1, '03_COMPLEX': 2, '04_LYMPHO': 3, '05_DEBRIS': 4, '06_MUCOSA': 5, '07_ADIPOSE': 6, '08_EMPTY': 7}\n",
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 744 images belonging to 8 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3496 images belonging to 8 classes.\n",
            "Found 744 images belonging to 8 classes.\n",
            "Epoch 1/50\n",
            "219/219 [==============================] - 19s 83ms/step - loss: 1.9015 - accuracy: 0.4951 - val_loss: 0.9805 - val_accuracy: 0.6048\n",
            "Epoch 2/50\n",
            "219/219 [==============================] - 19s 85ms/step - loss: 1.0262 - accuracy: 0.6307 - val_loss: 0.7171 - val_accuracy: 0.7083\n",
            "Epoch 3/50\n",
            "219/219 [==============================] - 18s 81ms/step - loss: 0.9042 - accuracy: 0.6776 - val_loss: 0.5788 - val_accuracy: 0.7836\n",
            "Epoch 4/50\n",
            "219/219 [==============================] - 18s 84ms/step - loss: 0.8284 - accuracy: 0.7074 - val_loss: 0.5869 - val_accuracy: 0.8038\n",
            "Epoch 5/50\n",
            "219/219 [==============================] - 18s 84ms/step - loss: 0.7599 - accuracy: 0.7182 - val_loss: 0.4481 - val_accuracy: 0.8414\n",
            "Epoch 6/50\n",
            "219/219 [==============================] - 19s 87ms/step - loss: 0.7140 - accuracy: 0.7463 - val_loss: 0.4935 - val_accuracy: 0.8293\n",
            "Epoch 7/50\n",
            "219/219 [==============================] - 18s 83ms/step - loss: 0.6863 - accuracy: 0.7480 - val_loss: 0.4006 - val_accuracy: 0.8481\n",
            "Epoch 8/50\n",
            "219/219 [==============================] - 18s 81ms/step - loss: 0.6517 - accuracy: 0.7674 - val_loss: 0.3970 - val_accuracy: 0.8656\n",
            "Epoch 9/50\n",
            "219/219 [==============================] - 18s 81ms/step - loss: 0.6195 - accuracy: 0.7689 - val_loss: 0.4243 - val_accuracy: 0.8481\n",
            "Epoch 10/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.6178 - accuracy: 0.7829 - val_loss: 0.4088 - val_accuracy: 0.8642\n",
            "Epoch 11/50\n",
            "219/219 [==============================] - 19s 85ms/step - loss: 0.5823 - accuracy: 0.7952 - val_loss: 0.4152 - val_accuracy: 0.8616\n",
            "Epoch 12/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.5634 - accuracy: 0.7955 - val_loss: 0.6693 - val_accuracy: 0.7809\n",
            "Epoch 13/50\n",
            "219/219 [==============================] - 18s 84ms/step - loss: 0.5509 - accuracy: 0.8155 - val_loss: 0.4803 - val_accuracy: 0.8468\n",
            "Epoch 14/50\n",
            "219/219 [==============================] - 18s 81ms/step - loss: 0.5457 - accuracy: 0.8098 - val_loss: 0.4202 - val_accuracy: 0.8642\n",
            "Epoch 15/50\n",
            "219/219 [==============================] - 18s 81ms/step - loss: 0.5071 - accuracy: 0.8218 - val_loss: 0.6107 - val_accuracy: 0.8239\n",
            "Epoch 16/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.5264 - accuracy: 0.8275 - val_loss: 0.4630 - val_accuracy: 0.8616\n",
            "Epoch 17/50\n",
            "219/219 [==============================] - 18s 83ms/step - loss: 0.5110 - accuracy: 0.8229 - val_loss: 0.5206 - val_accuracy: 0.8575\n",
            "Epoch 18/50\n",
            "219/219 [==============================] - 18s 84ms/step - loss: 0.4831 - accuracy: 0.8298 - val_loss: 0.6126 - val_accuracy: 0.8427\n",
            "Epoch 19/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.4965 - accuracy: 0.8364 - val_loss: 0.4408 - val_accuracy: 0.8508\n",
            "Epoch 20/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.4619 - accuracy: 0.8347 - val_loss: 0.4173 - val_accuracy: 0.8965\n",
            "Epoch 21/50\n",
            "219/219 [==============================] - 19s 87ms/step - loss: 0.4997 - accuracy: 0.8395 - val_loss: 0.7074 - val_accuracy: 0.8562\n",
            "Epoch 22/50\n",
            "219/219 [==============================] - 18s 81ms/step - loss: 0.4348 - accuracy: 0.8495 - val_loss: 0.4550 - val_accuracy: 0.8817\n",
            "Epoch 23/50\n",
            "219/219 [==============================] - 19s 86ms/step - loss: 0.4775 - accuracy: 0.8444 - val_loss: 0.5257 - val_accuracy: 0.8629\n",
            "Epoch 24/50\n",
            "219/219 [==============================] - 21s 94ms/step - loss: 0.4447 - accuracy: 0.8535 - val_loss: 0.4474 - val_accuracy: 0.8683\n",
            "Epoch 25/50\n",
            "219/219 [==============================] - 18s 84ms/step - loss: 0.4346 - accuracy: 0.8564 - val_loss: 0.5047 - val_accuracy: 0.8696\n",
            "Epoch 26/50\n",
            "219/219 [==============================] - 18s 81ms/step - loss: 0.4426 - accuracy: 0.8487 - val_loss: 0.6054 - val_accuracy: 0.8548\n",
            "Epoch 27/50\n",
            "219/219 [==============================] - 18s 84ms/step - loss: 0.4193 - accuracy: 0.8676 - val_loss: 0.6005 - val_accuracy: 0.8629\n",
            "Epoch 28/50\n",
            "219/219 [==============================] - 18s 84ms/step - loss: 0.4029 - accuracy: 0.8647 - val_loss: 0.5860 - val_accuracy: 0.8750\n",
            "Epoch 29/50\n",
            "219/219 [==============================] - 18s 84ms/step - loss: 0.4335 - accuracy: 0.8610 - val_loss: 0.6709 - val_accuracy: 0.8548\n",
            "Epoch 30/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.4173 - accuracy: 0.8638 - val_loss: 0.5436 - val_accuracy: 0.8831\n",
            "Epoch 31/50\n",
            "219/219 [==============================] - 19s 85ms/step - loss: 0.3876 - accuracy: 0.8710 - val_loss: 0.5702 - val_accuracy: 0.8763\n",
            "Epoch 32/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.3951 - accuracy: 0.8741 - val_loss: 0.5555 - val_accuracy: 0.8737\n",
            "Epoch 33/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.4184 - accuracy: 0.8656 - val_loss: 0.6276 - val_accuracy: 0.8696\n",
            "Epoch 34/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.3938 - accuracy: 0.8770 - val_loss: 0.5230 - val_accuracy: 0.8777\n",
            "Epoch 35/50\n",
            "219/219 [==============================] - 17s 76ms/step - loss: 0.3998 - accuracy: 0.8767 - val_loss: 0.5528 - val_accuracy: 0.8656\n",
            "Epoch 36/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.3960 - accuracy: 0.8773 - val_loss: 0.5787 - val_accuracy: 0.8737\n",
            "Epoch 37/50\n",
            "219/219 [==============================] - 18s 81ms/step - loss: 0.3907 - accuracy: 0.8701 - val_loss: 0.6492 - val_accuracy: 0.8642\n",
            "Epoch 38/50\n",
            "219/219 [==============================] - 19s 85ms/step - loss: 0.3744 - accuracy: 0.8922 - val_loss: 0.7617 - val_accuracy: 0.8427\n",
            "Epoch 39/50\n",
            "219/219 [==============================] - 18s 84ms/step - loss: 0.3827 - accuracy: 0.8896 - val_loss: 0.6612 - val_accuracy: 0.8723\n",
            "Epoch 40/50\n",
            "219/219 [==============================] - 18s 83ms/step - loss: 0.3930 - accuracy: 0.8873 - val_loss: 0.6013 - val_accuracy: 0.8723\n",
            "Epoch 41/50\n",
            "219/219 [==============================] - 18s 83ms/step - loss: 0.3696 - accuracy: 0.8856 - val_loss: 0.7092 - val_accuracy: 0.8790\n",
            "Epoch 42/50\n",
            "219/219 [==============================] - 18s 80ms/step - loss: 0.3589 - accuracy: 0.8893 - val_loss: 0.7134 - val_accuracy: 0.8656\n",
            "Epoch 43/50\n",
            "219/219 [==============================] - 17s 79ms/step - loss: 0.4005 - accuracy: 0.8787 - val_loss: 0.9483 - val_accuracy: 0.8199\n",
            "Epoch 44/50\n",
            "219/219 [==============================] - 16s 72ms/step - loss: 0.3423 - accuracy: 0.8896 - val_loss: 0.6690 - val_accuracy: 0.8616\n",
            "Epoch 45/50\n",
            "219/219 [==============================] - 18s 81ms/step - loss: 0.3687 - accuracy: 0.8930 - val_loss: 0.7323 - val_accuracy: 0.8763\n",
            "Epoch 46/50\n",
            "219/219 [==============================] - 18s 82ms/step - loss: 0.4165 - accuracy: 0.8781 - val_loss: 0.7535 - val_accuracy: 0.8602\n",
            "Epoch 47/50\n",
            "219/219 [==============================] - 19s 87ms/step - loss: 0.3515 - accuracy: 0.9010 - val_loss: 0.7264 - val_accuracy: 0.8777\n",
            "Epoch 48/50\n",
            "219/219 [==============================] - 19s 85ms/step - loss: 0.3771 - accuracy: 0.8916 - val_loss: 0.9556 - val_accuracy: 0.8642\n",
            "Epoch 49/50\n",
            "219/219 [==============================] - 19s 85ms/step - loss: 0.3807 - accuracy: 0.8893 - val_loss: 0.7411 - val_accuracy: 0.8763\n",
            "Epoch 50/50\n",
            "219/219 [==============================] - 18s 83ms/step - loss: 0.3502 - accuracy: 0.8953 - val_loss: 0.8022 - val_accuracy: 0.8669\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 0.8022 - accuracy: 0.8669\n",
            "[INFO] accuracy: 86.69%\n",
            "[INFO] Loss: 0.8021926283836365\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bae72a75d632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0msave_bottlebeck_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m \u001b[0mtrain_top_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bae72a75d632>\u001b[0m in \u001b[0;36mtrain_top_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acccuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_acccuracy'"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "'''\n",
        "Using Bottleneck Features for Multi-Class Classification in Keras\n",
        "We use this technique to build powerful (high accuracy without overfitting) Image Classification systems with small\n",
        "amount of training data.\n",
        "\n",
        "The code was tested on Python 3.5, with the following library versions,\n",
        "Keras 2.0.6\n",
        "TensorFlow 1.2.1\n",
        "OpenCV 3.2.0\n",
        "'''\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from tensorflow.keras import applications\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import cv2\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
        "train_data_dir = '/content/datasetaftersplit/train/'\n",
        "validation_data_dir = '/content/datasetaftersplit/val/'\n",
        "\n",
        "# number of epochs to train top model\n",
        "epochs = 50\n",
        "# batch size used by flow_from_directory and predict_generator\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "def save_bottlebeck_features():\n",
        "    # build the VGG19 network\n",
        "    model = applications.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    print(len(generator.filenames))\n",
        "    print(generator.class_indices)\n",
        "    print(len(generator.class_indices))\n",
        "\n",
        "    nb_train_samples = len(generator.filenames)\n",
        "    num_classes = len(generator.class_indices)\n",
        "\n",
        "    predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
        "\n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, predict_size_train)\n",
        "\n",
        "    np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    nb_validation_samples = len(generator.filenames)\n",
        "\n",
        "    predict_size_validation = int(\n",
        "        math.ceil(nb_validation_samples / batch_size))\n",
        "\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, predict_size_validation)\n",
        "\n",
        "    np.save('bottleneck_features_validation.npy',\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    datagen_top = ImageDataGenerator(rescale=1. / 255)\n",
        "    generator_top = datagen_top.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "    nb_train_samples = len(generator_top.filenames)\n",
        "    num_classes = len(generator_top.class_indices)\n",
        "\n",
        "    # save the class indices to use use later in predictions\n",
        "    np.save('class_indices.npy', generator_top.class_indices)\n",
        "\n",
        "    # load the bottleneck features saved earlier\n",
        "    train_data = np.load('bottleneck_features_train.npy')\n",
        "\n",
        "    # get the class lebels for the training data, in the original order\n",
        "    train_labels = generator_top.classes\n",
        "\n",
        "    # https://github.com/fchollet/keras/issues/3467\n",
        "    # convert the training labels to categorical vectors\n",
        "    train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "\n",
        "    generator_top = datagen_top.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "\n",
        "    nb_validation_samples = len(generator_top.filenames)\n",
        "\n",
        "    validation_data = np.load('bottleneck_features_validation.npy')\n",
        "\n",
        "    validation_labels = generator_top.classes\n",
        "    validation_labels = to_categorical(\n",
        "        validation_labels, num_classes=num_classes)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(train_data, train_labels,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(validation_data, validation_labels))\n",
        "\n",
        "    model.save_weights(top_model_weights_path)\n",
        "\n",
        "    (eval_loss, eval_accuracy) = model.evaluate(\n",
        "        validation_data, validation_labels, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))\n",
        "    print(\"[INFO] Loss: {}\".format(eval_loss))\n",
        "\n",
        "    plt.figure(1)\n",
        "\n",
        "    # summarize history for accuracy\n",
        "\n",
        "    plt.subplot(211)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_acccuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "    # summarize history for loss\n",
        "\n",
        "    plt.subplot(212)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def predict():\n",
        "    # load the class_indices saved in the earlier step\n",
        "    class_dictionary = np.load('class_indices.npy').item()\n",
        "\n",
        "    num_classes = len(class_dictionary)\n",
        "\n",
        "    # add the path to your test image below\n",
        "    image_path = 'path/to/your/test_image'\n",
        "\n",
        "    orig = cv2.imread(image_path)\n",
        "\n",
        "    print(\"[INFO] loading and preprocessing image...\")\n",
        "    image = load_img(image_path, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "\n",
        "    # important! otherwise the predictions will be '0'\n",
        "    image = image / 255\n",
        "\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    # build the VGG16 network\n",
        "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
        "\n",
        "    # get the bottleneck prediction from the pre-trained VGG16 model\n",
        "    bottleneck_prediction = model.predict(image)\n",
        "\n",
        "    # build top model\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=bottleneck_prediction.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "    model.load_weights(top_model_weights_path)\n",
        "\n",
        "    # use the bottleneck prediction on the top model to get the final\n",
        "    # classification\n",
        "    class_predicted = model.predict_classes(bottleneck_prediction)\n",
        "\n",
        "    probabilities = model.predict_proba(bottleneck_prediction)\n",
        "\n",
        "    inID = class_predicted[0]\n",
        "\n",
        "    inv_map = {v: k for k, v in class_dictionary.items()}\n",
        "\n",
        "    label = inv_map[inID]\n",
        "\n",
        "    # get the prediction label\n",
        "    print(\"Image ID: {}, Label: {}\".format(inID, label))\n",
        "\n",
        "    # display the predictions with the image\n",
        "    cv2.putText(orig, \"Predicted: {}\".format(label), (10, 30),\n",
        "                cv2.FONT_HERSHEY_PLAIN, 1.5, (43, 99, 255), 2)\n",
        "\n",
        "    cv2.imshow(\"Classification\", orig)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "save_bottlebeck_features()\n",
        "train_top_model()\n",
        "predict()\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "SOURCE:\n",
        "\n",
        "http://www.codesofinterest.com/2017/08/bottleneck-features-multi-class-classification-keras.html"
      ],
      "metadata": {
        "id": "U-N7mZGgoWXT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvDZ5ZVlXhX8UbUemfVYmJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}